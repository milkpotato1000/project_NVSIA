{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01791035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# llm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# db\n",
    "import psycopg2\n",
    "\n",
    "# tfidf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb18106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일 로드\n",
    "load_dotenv()  # 현재 디렉토리 내 .env 파일 정보를 환경변수로 읽어옴\n",
    "\n",
    "# 환경변수 확인\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found. .env 또는 시스템 환경변수에 설정하세요.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "llm_model = \"gpt-4o-mini\"\n",
    "embed_model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903fee4",
   "metadata": {},
   "source": [
    "### **[기존 함수]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb7c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_to_strCSV(value):\n",
    "    \"\"\"\n",
    "    리스트 또는 쉼표 문자열을 'a, b, c' 형태의 문자열로 표준화\n",
    "    \"\"\"\n",
    "    if not value:\n",
    "        return \"\"\n",
    "\n",
    "    # 이미 리스트이면 → 요소 strip 후 join\n",
    "    if isinstance(value, list):\n",
    "        return \", \".join(x.strip() for x in value)\n",
    "\n",
    "    # 문자열이면 split → 다시 join 처리\n",
    "    if isinstance(value, str):\n",
    "        return \", \".join(x.strip() for x in value.split(\",\"))\n",
    "\n",
    "    # 그 외 타입은 문자열로 강제 변환\n",
    "    return str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df630a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_summary(title, contents, publish_date, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    뉴스 기사를 LLM으로 요약하고 항목별 데이터 반환\n",
    "    \n",
    "    Change Log:\n",
    "        @@ 11.18 \n",
    "        > 프롬프트 수정\n",
    "            - 키값 정리 형식 재 설정 - 데이터 프레임의 컬럼명이 키값이 되도록 수정\n",
    "            - 추가적인 컬럼 event_person 추가\n",
    "            - event_obj 컬럼명 event_org로 변경\n",
    "        @@ 11.19\n",
    "        > 프롬프트 수정\n",
    "            - 인물명은 이름만 명확하게\n",
    "            - 지역명은 [국가, 도, 시] 단위로 명확하게\n",
    "            - 평양 쌀, 옥수수, 달러환율 정보는 각기 별개의 데이터로 저장\n",
    "            - 키워드는 인물명, 지역명을 반드시 포함 + 요약내용 대표 단어 추가\n",
    "        > result 파싱 방식 수정\n",
    "            - eval()에서 jason.load()로 변경\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    아래 기사를 분석하여 요구된 정보를 작성하시오.\n",
    "\n",
    "    # 기사 제목:\n",
    "    {title}\n",
    "\n",
    "    # 기사 내용:\n",
    "    {contents}\n",
    "\n",
    "    # 기사 작성일:\n",
    "    {publish_date}\n",
    "\n",
    "   1. 아래 형식으로 정리 (괄호안 각 key값의 한글 설명은 참고만 하고 최종 결과에는 포함하지 않음)\n",
    "    - summary(주요 사건 요약):\n",
    "    - event_title(사건 주제):\n",
    "    - event_date(사건 발생일):\n",
    "    - event_person(사건 핵심 인물):\n",
    "    - event_org(사건 핵심 조직/기관):\n",
    "    - event_loc(사건 발생 지명):\n",
    "    - keywords(주요 키워드):\n",
    "    - p_rice(won/kg)(쌀 가격):\n",
    "    - p_corn(won/kg)(옥수수 가격):\n",
    "    - p_usd(won/usd)(달러 가격):\n",
    "    \n",
    "    2. 각 카테고리의 조건\n",
    "    - \"summary\": 3 문장 이하로 핵심 내용만 발췌.\n",
    "    - \"event_title\": 간단한 한 문장으로 사건 주제 작성.\n",
    "    - \"event_date\": yyyy-mm-dd 형식, 기사에 \"event_date\"가 명시되지 않았으면 \"기사 내용\" 중 시간 또는 기간을 나타내는 단어(예시로, '어제', '사흘전', '일주일 전' 등)를 참고하여 \"기사 작성일\" 기준 계산.\n",
    "    - \"event_person\": 사건의 주체 인물(들)의 이름만 입력, 다수의 경우 쉼표로 구분.\n",
    "    - \"event_org\": 사건의 주체 조직 및 기관의 이름만 입력, 다수의 경우 쉼표로 구분, **언론사명은 반드시 제외**, **신문사명은 반드시 제외**, **기자가 참고한 출처의 이름도 반드시 제외**, **\"노동신문\"은 반드시 제외**.\n",
    "    - \"event_loc\": [도, 시]단위 지명만을 입력하되 \"도\" 와 \"시\" 정보가 함께 있는 경우는 반드시 행적구역별로 분리해서 입력. 건물등에서 일어난 사건의 경우는 해당 장소의 [도, 시] 지명을 입력, 행정구역이 \"시\"일 경우는 꼭 \"시\"를 명시 (개성시, 평양시, 고성시 등). \n",
    "    특히 \"평양\" / \"평양직할시\" / \"평양시\"와 같이 한 지명에 다양한 표기가 있을경우는 \"평양시\" ([시 이름] + 시)와 같은 형태로 통일. **\"북한\" 이라는 단어는 반드시 제외**. 북한이 아닌 해외의 사건의 경우만 국가명을 입력.\n",
    "    - \"keywords\": \"summary\", \"event_title\", \"event_person\", \"event_org\", \"event_loc\" 모두를 종합적으로 고려하여 해당 뉴스 사건을 대표할 수 있는 **단어 5개 선정**, **\"북한\" 이라는 단어는 반드시 제외**, 쉼표로 구분하여 입력.\n",
    "    - p_rice(won/kg): 반드시 **평양의 \"쌀 가격\"**이 확실하게 명시되어있는 경우만 추출, **숫자만** 입력. \"평양\"의 물가 정보만을 추출하기 위함. \"평양\"의 쌀 가격이라는 근거가 100% 명확하지 않은 경우 **절대로 지어내지 말고 빈칸**으로 남김. \n",
    "    - p_corn(won/kg): 반드시 **평양의 \"옥수수가격\"**이 확실하게 명시되어있는 경우만 추출, **숫자만** 입력. \"평양\"의 물가 정보만을 추출하기 위함. \"평양\"의 옥수수 가격이라는 근거가 100% 명확하지 않은 경우 **절대로 지어내지 말고 빈칸**으로 남김. \n",
    "    - p_usd(won/usd): 반드시 **평양의 \"달러 가격\"**이 확실하게 명시되어있는 경우만 추출, **숫자만** 입력. \"평양\"의 물가 정보만을 추출하기 위함. \"평양\"의 달러 환율이라는 근거가 100% 명확하지 않은 경우 **절대로 지어내지 말고 빈칸**으로 남김.\n",
    "    \n",
    "    - 위 결과를 종합하여 딕셔너리 형태로 출력.\n",
    "    - 결과를 출력하기 전 다음 체크리스트를 스스로 검증하라:\n",
    "        - [ ] 내가 사용한 모든 답과 수치는 기사 원문에 존재한다.\n",
    "        - [ ] 서로 다른 가격(예: 식용유 vs 쌀 등)을 혼동하지 않았다.\n",
    "        \n",
    "    - 설명 출력 금지, 답만 출력.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 북한 관련 뉴스 사건 정보를 추출하는 전문 분석 모델입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # 소모 토큰량 추출\n",
    "    # input_tokens = response.usage.prompt_tokens\n",
    "    # output_tokens = response.usage.completion_tokens\n",
    "\n",
    "    # LLM 결과 출력 가져오기\n",
    "    result_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    # 문자열을 dict로 변환\n",
    "    try:\n",
    "        result = json.loads(result_text)  # json 파싱으로 수정(11.19)\n",
    "    except:\n",
    "        print(\"Parsing error:\", result_text)\n",
    "        return None\n",
    "\n",
    "    # return result, input_tokens, output_tokens\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a05ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_df(df, cache_file=\"cache_output.csv\", model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    뉴스 요약 파싱 정보를 DataFrame으로 변환\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 원본 뉴스 데이터프레임. 최소 컬럼 'id', 'title', 'contents', 'publish_date' 포함.\n",
    "        cache_file (str, optional): 요약 결과를 저장/로드할 캐시 파일명. Defaults to \"cache_output.csv\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 뉴스 요약 및 사건 정보가 포함된 DataFrame. \n",
    "                      컬럼:\n",
    "                        - 'id': 뉴스 ID\n",
    "                        - 'summary': 주요 사건 요약 (문자열)\n",
    "                        - 'keywords': 주요 키워드 (리스트)\n",
    "                        - 'event_title': 사건 제목 (문자열)\n",
    "                        - 'event_date': 사건 발생일 (yyyy-mm-dd)\n",
    "                        - 'event_person': 사건 핵심 인물 (리스트)\n",
    "                        - 'event_org': 사건 핵심 조직/기관 (리스트)\n",
    "                        - 'event_loc': 사건 발생 지명 (리스트)\n",
    "                        - 'p_rice(won/kg)': 평양 쌀 가격 (정수)\n",
    "                        - 'p_corn(won/kg)': 평양 옥수수 가격 (정수)\n",
    "                        - 'p_usd(won/usd)': 원/달러 환율 (정수)\n",
    "                        - 'job_cost': 처리 비용 추정 (float)\n",
    "    \n",
    "    Example:\n",
    "        >>> df_summary = get_article_summary(news_df)\n",
    "        >>> df_summary.head()\n",
    "\n",
    "    Change Log:\n",
    "        @@ 11.19\n",
    "        - 캐싱 기능 추가: 뉴스 id 기반으로 이미 요약된 뉴스는 스킵하도록 수정.\n",
    "        - 새로운 뉴스가 추가될 경우에만 LLM 호출 수행.\n",
    "        - 쉼표로 구분된 항목(event_person, event_org, event_loc, keywords)을 리스트 형태로 변환.\n",
    "        @@ 11.20\n",
    "        - content 줄바꿈, 공백 문자 공백(' ')으로 처리 (실패, 원하는 결과를 얻지 못함)\n",
    "        - [id 조회 --> df 필터 -->> 판다스 시리즈로 전달] 방식으로 수정 (정보가 제대로 추출되지 않는듯함.)\n",
    "            > 해당 방식으로 진행시 아무런 가격 정보가 추출되지 않았다.\n",
    "        - 다시 원래 content 줄바꿈, 공백 문자 공백(' ')으로 처리 방식으로 돌아옴.\n",
    "        - \"gpt-4.1-nano\" 테스트 -> 가격 혼동 문제는 해결\n",
    "        - 함수에 model 선택 파라미터 추가\n",
    "    \"\"\"\n",
    "    \n",
    "########## 코드 시작 ##########\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 0. 기본 설정 / 스키마 정의\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    new_cols = [\n",
    "        \"summary\", \"keywords\", \"event_title\", \"event_date\", \"event_person\",\n",
    "        \"event_org\", \"event_loc\", \"p_rice(won/kg)\", \"p_corn(won/kg)\",\n",
    "        \"p_usd(won/usd)\", #\"job_cost\"\n",
    "    ]\n",
    "\n",
    "    cache_dir = \"data/cached\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_file_path = os.path.join(cache_dir, cache_file)\n",
    "\n",
    "    # 스키마 정의 (dtype 안정화)\n",
    "    schema = {\n",
    "        \"id\": pd.Series(dtype=\"object\"),\n",
    "        \"summary\": pd.Series(dtype=\"object\"),\n",
    "        \"keywords\": pd.Series(dtype=\"object\"),\n",
    "        \"event_title\": pd.Series(dtype=\"object\"),\n",
    "        \"event_date\": pd.Series(dtype=\"object\"),\n",
    "        \"event_person\": pd.Series(dtype=\"object\"),\n",
    "        \"event_org\": pd.Series(dtype=\"object\"),\n",
    "        \"event_loc\": pd.Series(dtype=\"object\"),\n",
    "        \"p_rice(won/kg)\": pd.Series(dtype=\"Int64\"),\n",
    "        \"p_corn(won/kg)\": pd.Series(dtype=\"Int64\"),\n",
    "        \"p_usd(won/usd)\": pd.Series(dtype=\"Int64\"),\n",
    "        # \"job_cost\": pd.Series(dtype=\"float\"),\n",
    "    }\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 1. 캐시 로드 or 초기화\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    if os.path.exists(cache_file_path):\n",
    "        print(\"⏳ 캐시 파일 로드 중...\")\n",
    "        output_df = pd.read_csv(cache_file_path)\n",
    "\n",
    "        # schema 기준으로 dtype 강제 적용\n",
    "        for col, series in schema.items():\n",
    "            if col not in output_df.columns:\n",
    "                output_df[col] = series  # 없는 컬럼은 빈 Series 생성\n",
    "            else:\n",
    "                output_df[col] = output_df[col].astype(series.dtype, errors=\"ignore\")\n",
    "\n",
    "        print(\"✔ 캐시 반영 완료!\")\n",
    "    else:\n",
    "        print(\"캐시 없음 → 빈 DF 생성\")\n",
    "        output_df = pd.DataFrame(schema)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 2. 신규 기사만 처리\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    cached_ids = set(output_df[\"id\"].astype(str).tolist())\n",
    "    new_articles = df[~df[\"id\"].astype(str).isin(cached_ids)]\n",
    "\n",
    "    print(f\"신규 샘플 발견: {len(new_articles)}개\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 3. 루프 처리 / row 1개씩 추가 \n",
    "    # ---------------------------------------------------------------------------------\n",
    "    for cnt, (i, row) in enumerate(new_articles.iterrows()): \n",
    "        article_id = row['id']\n",
    "        title = row['title']\n",
    "        contents = row['contents']\n",
    "        publish_date = row['publish_date']\n",
    "\n",
    "        print(f\"Summarizing [{article_id}] ...\")\n",
    "\n",
    "        # LLM 호출\n",
    "        result, input_tokens, output_tokens = get_article_summary(title, contents, publish_date, model)\n",
    "\n",
    "        # 새 row 구성 (None → pd.NA)\n",
    "        new_data = {\n",
    "            \"id\": article_id,\n",
    "            \"summary\": result.get(\"summary\"),\n",
    "            \"keywords\": value_to_strCSV(result.get(\"keywords\")),\n",
    "            \"event_title\": result.get(\"event_title\"),\n",
    "            \"event_date\": result.get(\"event_date\"),\n",
    "            \"event_person\": value_to_strCSV(result.get(\"event_person\")),\n",
    "            \"event_org\": value_to_strCSV(result.get(\"event_org\")),\n",
    "            \"event_loc\": value_to_strCSV(result.get(\"event_loc\")),\n",
    "            \"p_rice(won/kg)\": pd.NA if result.get(\"p_rice(won/kg)\")==0 or result.get(\"p_rice(won/kg)\") is None else result.get(\"p_rice(won/kg)\"),\n",
    "            \"p_corn(won/kg)\": pd.NA if result.get(\"p_rice(won/kg)\")==0 or result.get(\"p_corn(won/kg)\") is None else result.get(\"p_corn(won/kg)\"),\n",
    "            \"p_usd(won/usd)\": pd.NA if result.get(\"p_usd(won/usd)\")==0 or result.get(\"p_usd(won/usd)\") is None else result.get(\"p_usd(won/usd)\"),\n",
    "            \"job_cost\": (\n",
    "                input_tokens * 0.15 / 1_000_000 +\n",
    "                output_tokens * 0.60 / 1_000_000\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # 컬럼 순서와 dtype 맞춰서 row 추가\n",
    "        new_data_aligned = {col: new_data.get(col, pd.NA) for col in output_df.columns}\n",
    "        \n",
    "        new_row_df = pd.DataFrame([new_data_aligned]).dropna(axis=1, how='all')  # dict → DataFrame 변환, na값이 있는 컬럼 제거\n",
    "        output_df = pd.concat([output_df, new_row_df], ignore_index=True)\n",
    "\n",
    "        # 캐시 저장\n",
    "        output_df.to_csv(cache_file_path, index=False)\n",
    "\n",
    "        print(f\"Job Complete! [{article_id}] ({cnt+1}/{len(new_articles)})\")\n",
    "\n",
    "    print(\"전체 작업 완료!\")\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9fc2c",
   "metadata": {},
   "source": [
    "### **[LLM > DB 함수]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2b41817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMtoDatabase:\n",
    "\n",
    "    def __init__(self, host, database, user, password, port, tfidf_vectorizer_path, svm_model_path, label_encoder_path):\n",
    "        \"\"\"\n",
    "\n",
    "        CSV(title, contents, publish_date, url) 파일을 받아 LLM 요약.\n",
    "        LLM ouput Postgre DB table에 저장.\n",
    "\n",
    "        [수정 2025-11-27]\n",
    "        1) CSV(title, contents, publish_date, url) 파일을 input으로 받음.\n",
    "        2) LLM summary, keywords 등등 출력.\n",
    "        3) summary, keywords TF-IDF 변환 > SVM > 카테고리 분류.\n",
    "        4) ADA embedding 진행 > db 저장. (postgres에서 l2 norm 진행)\n",
    "        5) Postgre DB table에 저장.\n",
    "\n",
    "        [수정 2025-11-28]\n",
    "        매개변수 tfidf_vectorizer_path, svm_model_path, label_encoder_path 추가.\n",
    "        '시', '도' 변환 코드 merge.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Postgre db 연결\n",
    "        self.conn = psycopg2.connect(host=host, database=database, user=user, password=password, port=port)\n",
    "        self.cur = self.conn.cursor()\n",
    "\n",
    "        # TF-IDF pickle 파일 로드\n",
    "        with open(tfidf_vectorizer_path, \"rb\") as f:\n",
    "            self.tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "        with open(svm_model_path, \"rb\") as f:\n",
    "            self.svm_model = pickle.load(f)\n",
    "\n",
    "        with open(label_encoder_path, \"rb\") as f:\n",
    "            self.label_encoder = pickle.load(f) \n",
    "\n",
    "        # Load nk_cities and build maps for normalization\n",
    "        try:\n",
    "            self.nk_cities = pd.read_csv('data/nk_cities.csv', encoding='euc-kr')\n",
    "            self.provinces_map, self.cities_map = self._build_maps()\n",
    "            self.BROAD_TERMS_MAP = {\n",
    "                \"평안도\": [\"평안남도\", \"평안북도\"],\n",
    "                \"함경도\": [\"함경남도\", \"함경북도\"],\n",
    "                \"황해도\": [\"황해남도\", \"황해북도\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load nk_cities.csv or build maps. Normalization will be skipped. Error: {e}\")\n",
    "            self.nk_cities = None\n",
    "            self.provinces_map = {}\n",
    "            self.cities_map = {}\n",
    "            self.BROAD_TERMS_MAP = {}      \n",
    "\n",
    "    def _get_search_keys(self, name):\n",
    "        \"\"\"\n",
    "\n",
    "        [추가 2025-11-28]\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if pd.isna(name): return [], None\n",
    "        # Handle parentheses: \"나선시(라선시)\" -> parts: [\"나선시\", \"라선시\"]\n",
    "        parts = re.split(r'[()]', name)\n",
    "        parts = [p.strip() for p in parts if p.strip()]\n",
    "        \n",
    "        canonical_name = parts[0] # The first part is the canonical name\n",
    "        \n",
    "        keys = []\n",
    "        for p in parts:\n",
    "            # Strip suffixes '도', '시', '군', '구역' for search key\n",
    "            key = p\n",
    "            if key.endswith('도'): key = key[:-1]\n",
    "            elif key.endswith('시'): key = key[:-1]\n",
    "            elif key.endswith('군'): key = key[:-1]\n",
    "            elif key.endswith('구역'): key = key[:-1]\n",
    "            keys.append(key)\n",
    "        return keys, canonical_name\n",
    "\n",
    "    def _build_maps(self):\n",
    "        \"\"\"\n",
    "\n",
    "        [추가 2025-11-28]\n",
    "        1) 북한 행정구역 정리 파일(nk_cities.csv) \n",
    "        2) 약어 혹은 확장명 매핑\n",
    "\n",
    "        \"\"\"\n",
    "        provinces_map = {} # search_key -> canonical_full_name\n",
    "        cities_map = {}    # search_key -> {'full': canonical_full_name, 'province': province_canonical_name}\n",
    "\n",
    "        for idx, row in self.nk_cities.iterrows():\n",
    "            # Process Province\n",
    "            p_keys, p_canon = self._get_search_keys(row['도'])\n",
    "            for k in p_keys:\n",
    "                provinces_map[k] = p_canon\n",
    "                \n",
    "            # Process City\n",
    "            c_keys, c_canon = self._get_search_keys(row['시'])\n",
    "            for k in c_keys:\n",
    "                cities_map[k] = {\n",
    "                    'full': c_canon,\n",
    "                    'province': p_canon # This might be None or a string\n",
    "                }\n",
    "\n",
    "        # Manual additions for abbreviations and broader terms\n",
    "        abbr_map = {\n",
    "            '평남': '평안남도',\n",
    "            '평북': '평안북도',\n",
    "            '함남': '함경남도',\n",
    "            '함북': '함경북도',\n",
    "            '황남': '황해남도',\n",
    "            '황북': '황해북도',\n",
    "            '양강': '양강도',\n",
    "            '자강': '자강도',\n",
    "            '강원': '강원도',\n",
    "            '평안도': '평안도', # Broader term\n",
    "            '황해도': '황해도', # Broader term\n",
    "            '함경도': '함경도',  # Broader term\n",
    "            '평안': '평안도' # Example 7: \"평안\" -> \"평안도\" (Assuming broader term)\n",
    "        }\n",
    "\n",
    "        for abbr, full in abbr_map.items():\n",
    "            provinces_map[abbr] = full\n",
    "            \n",
    "        return provinces_map, cities_map\n",
    "\n",
    "    def map_location_normalized(self, loc_str):\n",
    "        \"\"\"\n",
    "\n",
    "        [추가 2025-11-28]\n",
    "        rule-based 로 관리.\n",
    "        1) 광역시(남포시, 개성시, 나선시(라선시), 평양시)인 경우 '도' 없어도 표기.\n",
    "        2) 광역시가 아닌 '시'의 경우, 매핑된 '도'를 함께 표기.\n",
    "        3) '도' 항상 표기.\n",
    "\n",
    "        \"\"\"\n",
    "        if pd.isna(loc_str) or not isinstance(loc_str, str):\n",
    "            return None\n",
    "        \n",
    "        found_provinces = set()\n",
    "        found_cities = [] # List of dicts\n",
    "        \n",
    "        # 1. Search for Provinces\n",
    "        for key, full_name in self.provinces_map.items():\n",
    "            if key in loc_str:\n",
    "                found_provinces.add(full_name)\n",
    "                \n",
    "        # 2. Search for Cities\n",
    "        for key, info in self.cities_map.items():\n",
    "            if key in loc_str:\n",
    "                match_info = info.copy()\n",
    "                match_info['key'] = key\n",
    "                found_cities.append(match_info)\n",
    "                \n",
    "        # 3. Consolidate and Remove Redundancy\n",
    "        \n",
    "        # 3a. Identify implied provinces from found cities\n",
    "        implied_provinces = set()\n",
    "        for c in found_cities:\n",
    "            if pd.notna(c['province']):\n",
    "                implied_provinces.add(c['province'])\n",
    "                \n",
    "        # 3b. Remove found provinces if they are implied by the cities\n",
    "        temp_provinces = set()\n",
    "        for p in found_provinces:\n",
    "            if p not in implied_provinces:\n",
    "                temp_provinces.add(p)\n",
    "        \n",
    "        # 3c. Remove Broad Terms if Specific Terms are present\n",
    "        all_present_specific_provinces = temp_provinces.union(implied_provinces)\n",
    "        \n",
    "        final_provinces = set()\n",
    "        for p in temp_provinces:\n",
    "            is_redundant_broad = False\n",
    "            if p in self.BROAD_TERMS_MAP:\n",
    "                # Check if any specific term for this broad term is present\n",
    "                for specific in self.BROAD_TERMS_MAP[p]:\n",
    "                    if specific in all_present_specific_provinces:\n",
    "                        is_redundant_broad = True\n",
    "                        break\n",
    "            \n",
    "            if not is_redundant_broad:\n",
    "                final_provinces.add(p)\n",
    "                \n",
    "        # 4. Format Output\n",
    "        final_results = set()\n",
    "        \n",
    "        # Add Remaining Provinces\n",
    "        for p in final_provinces:\n",
    "            final_results.add(p)\n",
    "            \n",
    "        # Add Cities (Format: \"Province City\" or \"City\")\n",
    "        for c in found_cities:\n",
    "            full_city = c['full']\n",
    "            province = c['province']\n",
    "            \n",
    "            if pd.notna(province):\n",
    "                final_results.add(f\"{province} {full_city}\")\n",
    "            else:\n",
    "                final_results.add(full_city)\n",
    "                \n",
    "        if not final_results:\n",
    "            return None\n",
    "            \n",
    "        return ', '.join(sorted(list(final_results)))    \n",
    "\n",
    "\n",
    "    def get_article_summary(self, title, contents, publish_date):\n",
    "        \"\"\"\n",
    "        뉴스 기사를 LLM으로 요약하고 항목별 데이터 반환\n",
    "        \n",
    "        Change Log:\n",
    "            @@ 11.18 \n",
    "            > 프롬프트 수정\n",
    "                - 키값 정리 형식 재 설정 - 데이터 프레임의 컬럼명이 키값이 되도록 수정\n",
    "                - 추가적인 컬럼 event_person 추가\n",
    "                - event_obj 컬럼명 event_org로 변경\n",
    "            @@ 11.19\n",
    "            > 프롬프트 수정\n",
    "                - 인물명은 이름만 명확하게\n",
    "                - 지역명은 [국가, 도, 시] 단위로 명확하게\n",
    "                - 평양 쌀, 옥수수, 달러환율 정보는 각기 별개의 데이터로 저장\n",
    "                - 키워드는 인물명, 지역명을 반드시 포함 + 요약내용 대표 단어 추가\n",
    "            > result 파싱 방식 수정\n",
    "                - eval()에서 jason.load()로 변경\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "    아래 기사를 분석하여 요구된 정보를 작성하시오.\n",
    "\n",
    "    # 기사 제목:\n",
    "    {title}\n",
    "\n",
    "    # 기사 내용:\n",
    "    {contents}\n",
    "\n",
    "    # 기사 작성일:\n",
    "    {publish_date}\n",
    "\n",
    "   1. 아래 형식으로 정리 (괄호안 각 key값의 한글 설명은 참고만 하고 최종 결과에는 포함하지 않음)\n",
    "    - summary(주요 사건 요약):\n",
    "    - event_title(사건 주제):\n",
    "    - event_date(사건 발생일):\n",
    "    - event_person(사건 핵심 인물):\n",
    "    - event_org(사건 핵심 조직/기관):\n",
    "    - event_loc(사건 발생 지명):\n",
    "    - keywords(주요 키워드):\n",
    "    \n",
    "    2. 각 카테고리의 조건\n",
    "    - \"summary\": 3 문장 이하로 핵심 내용만 발췌.\n",
    "    - \"event_title\": 간단한 한 문장으로 사건 주제 작성.\n",
    "    - \"event_date\": yyyy-mm-dd 형식, 기사에 \"event_date\"가 명시되지 않았으면 \"기사 내용\" 중 시간 또는 기간을 나타내는 단어(예시로, '어제', '사흘전', '일주일 전' 등)를 참고하여 \"기사 작성일\" 기준 계산.\n",
    "    - \"event_person\": 사건의 주체 인물(들)의 이름만 입력, 다수의 경우 쉼표로 구분.\n",
    "    - \"event_org\": 사건의 주체 조직 및 기관의 이름만 입력, 다수의 경우 쉼표로 구분, **언론사명은 반드시 제외**, **신문사명은 반드시 제외**, **기자가 참고한 출처의 이름도 반드시 제외**, **\"노동신문\"은 반드시 제외**.\n",
    "    - \"event_loc\": [도, 시]단위 지명만을 입력하되 \"도\" 와 \"시\" 정보가 함께 있는 경우는 반드시 행적구역별로 분리해서 입력. 건물등에서 일어난 사건의 경우는 해당 장소의 [도, 시] 지명을 입력, 행정구역이 \"시\"일 경우는 꼭 \"시\"를 명시 (개성시, 평양시, 고성시 등). \n",
    "    특히 \"평양\" / \"평양직할시\" / \"평양시\"와 같이 한 지명에 다양한 표기가 있을경우는 \"평양시\" ([시 이름] + 시)와 같은 형태로 통일. **\"북한\" 이라는 단어는 반드시 제외**. 북한이 아닌 해외의 사건의 경우만 국가명을 입력.\n",
    "    - \"keywords\": \"summary\", \"event_title\", \"event_person\", \"event_org\", \"event_loc\" 모두를 종합적으로 고려하여 해당 뉴스 사건을 대표할 수 있는 **단어 5개 선정**, **\"북한\" 이라는 단어는 반드시 제외**, 쉼표로 구분하여 입력.\n",
    "    \n",
    "    - 위 결과를 종합하여 딕셔너리 형태로 출력.\n",
    "    - 결과를 출력하기 전 다음 체크리스트를 스스로 검증하라:\n",
    "        - [ ] 내가 사용한 모든 답과 수치는 기사 원문에 존재한다.\n",
    "        \n",
    "    - 설명 출력 금지, 답만 출력.\n",
    "    \"\"\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=llm_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 북한 관련 뉴스 사건 정보를 추출하는 전문 분석 모델입니다.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # 문자열을 dict로 변환\n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        try:\n",
    "            result = json.loads(result_text)  # json 파싱으로 수정(11.19)\n",
    "        except:\n",
    "            print(\"Parsing error:\", result_text)\n",
    "            return None\n",
    "              \n",
    "        return result\n",
    "        \n",
    "    def value_to_strCSV(self, value):\n",
    "        \"\"\"\n",
    "        리스트 또는 쉼표 문자열을 'a, b, c' 형태의 문자열로 표준화\n",
    "        \"\"\"\n",
    "        if not value:\n",
    "            return \"\"\n",
    "\n",
    "        # 이미 리스트이면 → 요소 strip 후 join\n",
    "        if isinstance(value, list):\n",
    "            return \", \".join(x.strip() for x in value)\n",
    "\n",
    "        # 문자열이면 split → 다시 join 처리\n",
    "        if isinstance(value, str):\n",
    "            return \", \".join(x.strip() for x in value.split(\",\"))\n",
    "\n",
    "        # 그 외 타입은 문자열로 강제 변환\n",
    "        return str(value)\n",
    "    \n",
    "    def insert_summary(self, llm, title, publish_date, url, category, embedding):\n",
    "        \"\"\"\n",
    "        \n",
    "        LLM output 과 원본 csv 파일의 title, publish_date, url 데이터를 postgre table에 저장\n",
    "\n",
    "        [수정 2025-11-24]\n",
    "        issue: 동일 csv 파일로 코드 재실행 시, 이미 db에 등록된 contents가 새로운 id로 재등록.\n",
    "        수정: 동일한 url이 재입력 시, pass \n",
    "        상세: query >> ON CONFLICT (url) DO NOTHING << 추가\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        query = \"\"\"\n",
    "            INSERT INTO summary\n",
    "                (summary, keywords, event_title, event_date,\n",
    "                 event_person, event_org, event_loc, url, title, publish_date, category, embedding)\n",
    "            VALUES\n",
    "                (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (url) DO NOTHING;\n",
    "        \"\"\"\n",
    "\n",
    "        values = (\n",
    "            llm.get(\"summary\"),\n",
    "            self.value_to_strCSV(llm.get(\"keywords\")),\n",
    "            llm.get(\"event_title\"),\n",
    "            llm.get(\"event_date\"),\n",
    "            self.value_to_strCSV(llm.get(\"event_person\")),\n",
    "            self.value_to_strCSV(llm.get(\"event_org\")),\n",
    "            self.value_to_strCSV(llm.get(\"event_loc\")),\n",
    "            url,\n",
    "            title, \n",
    "            publish_date,\n",
    "            category,\n",
    "            embedding.tolist(),\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            self.cur.execute(query, values)\n",
    "            self.conn.commit()\n",
    "\n",
    "            if self.cur.rowcount == 0:\n",
    "                print(f\"[DB INSERT ERROR] 이미 존재하는 기사입니다. url={url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            print(f\"[DB INSERT ERROR] url={url} ⇒ {e}\")\n",
    "\n",
    "    def check_url(self, url):\n",
    "        \"\"\"\n",
    "\n",
    "        [추가 2025-11-24]\n",
    "        issue: LLM 처리 후 url 중복 체크 시, LLM 토큰 낭비.\n",
    "        해결: LLM 처리 이전 url 사전 체크 함수 추가. \n",
    "        상세: \n",
    "        코드 재실행 시 동일한 기사가 새로운 ID를 부여받지 않도록 방지.\n",
    "        url을 조회하여 있는 경우 skip.\n",
    "\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "            SELECT COUNT(*) FROM summary\n",
    "            WHERE url = %s;\n",
    "        \"\"\"\n",
    "\n",
    "        self.cur.execute(query, (url,))\n",
    "        count = self.cur.fetchone()[0]\n",
    "\n",
    "        return count > 0\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        [추가 2025-11-27]\n",
    "        issue: SVM 모델 구동 위해 형태 동일하게 변환.\n",
    "        해결: s_news_categorizer 전처리 코드와 동일한 로직.\n",
    "\n",
    "        \"\"\"\n",
    "        if pd.isna(text): \n",
    "            return \"\"\n",
    "        text = str(text).lower() \n",
    "        text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', '', text) \n",
    "        return text\n",
    "    \n",
    "    def get_category(self, summary, keywords):\n",
    "        \"\"\"\n",
    "\n",
    "        [추가 2025-11-27]\n",
    "        issue: 모델 분류기가 tf-idf를 활용함에 따라 tf-idf 변환 필요.\n",
    "        해결: s_news_categorizer 전처리 코드 merge.\n",
    "        상세: s_news_categorizer.ipynb 동일. \n",
    "        \n",
    "        [수정 2025-11-28]\n",
    "        summary + keywords 를 input을 받는 vectorizer 로 변경.\n",
    "        vectorizer >> svm >> label encoding\n",
    "\n",
    "        \"\"\"\n",
    "        preprocessed_summary = self.preprocess_text(summary)\n",
    "        preprocessed_keywords = self.preprocess_text(keywords)\n",
    "        combined_text = preprocessed_summary + \" \" + preprocessed_keywords\n",
    "\n",
    "        X_combined = self.tfidf_vectorizer.transform([combined_text])\n",
    "\n",
    "        svm_pred = self.svm_model.predict(X_combined)[0]\n",
    "\n",
    "        category = self.label_encoder.inverse_transform([svm_pred])[0]\n",
    "        return category\n",
    "    \n",
    "    def text_to_embedding(self, text):\n",
    "        \"\"\"\n",
    "\n",
    "        [추가 2025-11-27]\n",
    "        issue: 모델 분류기는 tf-idf를 활용하지만, 추천시스템은 embedding 활용.\n",
    "        해결: embedding 코드 추가.\n",
    "        상세: summary + keywords 를 임베딩 후 수평합. 해당 값은 postgres에 저장.\n",
    "\n",
    "        [수정 2025-11-28]\n",
    "        issue: text_embedding 라이브러리와 파이썬 버전 충돌\n",
    "        해결: text_embeddings[0].embedding >> text_embeddings.data[0].embedding\n",
    "\n",
    "        \"\"\"\n",
    "        text = text\n",
    "        text_embeddings = client.embeddings.create(\n",
    "            model = embed_model,\n",
    "            input = text\n",
    "        )\n",
    "        embeddings = np.array(text_embeddings.data[0].embedding, dtype = np.float32)\n",
    "        return embeddings\n",
    "    \n",
    "    def get_embeddings(self, summary, keywords):\n",
    "        \"\"\"\n",
    "\n",
    "        [추가 2025-11-27]\n",
    "        issue: 모델 분류기는 tf-idf를 활용하지만, 추천시스템은 embedding 활용.\n",
    "        해결: embedding 코드 추가.\n",
    "        상세: text_to_embedding 함수 받아와 summary, keywords 임베딩.\n",
    "\n",
    "        \"\"\"\n",
    "        embed_summary = self.text_to_embedding(summary)\n",
    "        embed_keywords = self.text_to_embedding(keywords)\n",
    "        embed_rec = np.hstack([embed_summary, embed_keywords])\n",
    "        return embed_rec\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        self.cur.close()\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\MODULABS\\Aiffelthon\\project_NVSIA\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\MODULABS\\Aiffelthon\\project_NVSIA\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\MODULABS\\Aiffelthon\\project_NVSIA\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\MODULABS\\Aiffelthon\\project_NVSIA\\.venv\\Lib\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[저장] 행 업로드 되었습니다. 0\n",
      "[저장] 행 업로드 되었습니다. 1\n",
      "[저장] 행 업로드 되었습니다. 2\n",
      "[저장] 행 업로드 되었습니다. 3\n",
      "[저장] 행 업로드 되었습니다. 4\n",
      "[저장] 행 업로드 되었습니다. 5\n",
      "[저장] 행 업로드 되었습니다. 6\n",
      "[저장] 행 업로드 되었습니다. 7\n",
      "[저장] 행 업로드 되었습니다. 8\n",
      "[저장] 행 업로드 되었습니다. 9\n",
      "[저장] 행 업로드 되었습니다. 10\n",
      "[저장] 행 업로드 되었습니다. 11\n",
      "[저장] 행 업로드 되었습니다. 12\n",
      "[저장] 행 업로드 되었습니다. 13\n",
      "[저장] 행 업로드 되었습니다. 14\n",
      "[저장] 행 업로드 되었습니다. 15\n",
      "[저장] 행 업로드 되었습니다. 16\n",
      "[저장] 행 업로드 되었습니다. 17\n",
      "[저장] 행 업로드 되었습니다. 18\n",
      "[저장] 행 업로드 되었습니다. 19\n",
      "[저장] 행 업로드 되었습니다. 20\n",
      "[저장] 행 업로드 되었습니다. 21\n",
      "[저장] 행 업로드 되었습니다. 22\n",
      "[저장] 행 업로드 되었습니다. 23\n",
      "[저장] 행 업로드 되었습니다. 24\n",
      "[종료] 모든 업로드가 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 실행 코드\n",
    "\n",
    "df = pd.read_csv(\"data/test_region_parsing.csv\", encoding=\"cp949\")  \n",
    "# 반드시 포함: title, contents, publish_date, url\n",
    "\n",
    "llm_db = LLMtoDatabase(\n",
    "    host=\"localhost\",\n",
    "    database=\"nvisiaDb\",\n",
    "    user=\"postgres\",\n",
    "    password=\"postgres1202\",\n",
    "    port=5432,\n",
    "    tfidf_vectorizer_path = \"pickle/s_news_cate_tfidf_xcombined_vec.pkl\", \n",
    "    svm_model_path = \"pickle/s_news_cate_model.pkl\", \n",
    "    label_encoder_path = \"pickle/s_news_cate_label_en.pkl\"\n",
    ")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    title = str(row[\"title\"])\n",
    "    contents = str(row[\"contents\"])\n",
    "    publish_date = str(row[\"publish_date\"])\n",
    "    url = str(row[\"url\"])\n",
    "\n",
    "    if llm_db.check_url(url):\n",
    "        print(f\"[중복] 이미 존재하는 기사입니다. 이어서 다음 기사를 분석합니다.\")\n",
    "        continue\n",
    "\n",
    "    # LLM \n",
    "    llm_output = llm_db.get_article_summary(title, contents, publish_date)\n",
    "\n",
    "    if llm_output is None:\n",
    "        print(f\"[에러] 데이터가 누락되어 다음 행으로 넘어갑니다. {idx}\")\n",
    "        continue\n",
    "\n",
    "    # event_loc 정규화\n",
    "    raw_loc = llm_output.get(\"event_loc\")\n",
    "    norm_loc = llm_db.map_location_normalized(raw_loc)\n",
    "    llm_output[\"event_loc\"] = norm_loc \n",
    "\n",
    "    # category 분류\n",
    "    summary_text = llm_output.get(\"summary\")\n",
    "    keywords_text = llm_output.get(\"keywords\")\n",
    "    category = llm_db.get_category(summary_text, keywords_text)\n",
    "\n",
    "    # embedding\n",
    "    embedding = llm_db.get_embeddings(summary_text, keywords_text)\n",
    "\n",
    "    # DB 저장\n",
    "    llm_db.insert_summary(llm_output, title, publish_date, url, category, embedding)\n",
    "\n",
    "    print(f\"[저장] 행 업로드 되었습니다. {idx}\")\n",
    "\n",
    "llm_db.close()\n",
    "print(\"[종료] 모든 업로드가 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dd24c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-nvsia-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
