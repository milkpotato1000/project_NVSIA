{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Normalization for full_df_final.csv\n",
    "\n",
    "This notebook applies the location normalization rules from `LLMtoDatabase.py` to the `event_loc` column of `full_df_final.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationNormalizer:\n",
    "    def __init__(self, cities_csv_path='data/nk_cities.csv'):\n",
    "        # Load nk_cities and build maps for normalization\n",
    "        try:\n",
    "            self.nk_cities = pd.read_csv(cities_csv_path, encoding='euc-kr')\n",
    "            self.BROAD_TERMS_MAP = {\n",
    "                \"평안도\": [\"평안남도\", \"평안북도\"],\n",
    "                \"함경도\": [\"함경남도\", \"함경북도\"],\n",
    "                \"황해도\": [\"황해남도\", \"황해북도\"]\n",
    "            }\n",
    "            self.provinces_map, self.cities_map = self._build_maps()\n",
    "            print(\"Normalization maps built successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load nk_cities.csv or build maps. Normalization will be skipped. Error: {e}\")\n",
    "            self.nk_cities = None\n",
    "            self.provinces_map = {}\n",
    "            self.cities_map = {}\n",
    "            self.BROAD_TERMS_MAP = {}\n",
    "\n",
    "    def _get_search_keys(self, name):\n",
    "        if pd.isna(name): return [], None\n",
    "        # Handle parentheses: \"나선시(라선시)\" -> parts: [\"나선시\", \"라선시\"]\n",
    "        parts = re.split(r'[()]', name)\n",
    "        parts = [p.strip() for p in parts if p.strip()]\n",
    "        \n",
    "        canonical_name = parts[0] # The first part is the canonical name\n",
    "        \n",
    "        keys = []\n",
    "        for p in parts:\n",
    "            # Strip suffixes '도', '시', '군', '구역' for search key\n",
    "            key = p\n",
    "            if key.endswith('도'): key = key[:-1]\n",
    "            elif key.endswith('시'): key = key[:-1]\n",
    "            elif key.endswith('군'): key = key[:-1]\n",
    "            elif key.endswith('구역'): key = key[:-1]\n",
    "            keys.append(key)\n",
    "        return keys, canonical_name\n",
    "\n",
    "    def _build_maps(self):\n",
    "        provinces_map = {} # search_key -> canonical_full_name\n",
    "        cities_map = {}    # search_key -> {'full': canonical_full_name, 'province': province_canonical_name}\n",
    "\n",
    "        for idx, row in self.nk_cities.iterrows():\n",
    "            # Process Province\n",
    "            p_keys, p_canon = self._get_search_keys(row['도'])\n",
    "            for k in p_keys:\n",
    "                provinces_map[k] = p_canon\n",
    "                \n",
    "            # Process City\n",
    "            c_keys, c_canon = self._get_search_keys(row['시'])\n",
    "            for k in c_keys:\n",
    "                cities_map[k] = {\n",
    "                    'full': c_canon,\n",
    "                    'province': p_canon # This might be None or a string\n",
    "                }\n",
    "\n",
    "        # Manual additions for abbreviations and broader terms\n",
    "        abbr_map = {\n",
    "            '평남': '평안남도',\n",
    "            '평북': '평안북도',\n",
    "            '함남': '함경남도',\n",
    "            '함북': '함경북도',\n",
    "            '황남': '황해남도',\n",
    "            '황북': '황해북도',\n",
    "            '양강': '양강도',\n",
    "            '자강': '자강도',\n",
    "            '강원': '강원도',\n",
    "            '평안도': '평안도', # Broader term\n",
    "            '황해도': '황해도', # Broader term\n",
    "            '함경도': '함경도',  # Broader term\n",
    "            '평안': '평안도' # Example 7: \"평안\" -> \"평안도\" (Assuming broader term)\n",
    "        }\n",
    "\n",
    "        for abbr, full in abbr_map.items():\n",
    "            provinces_map[abbr] = full\n",
    "            \n",
    "        return provinces_map, cities_map\n",
    "\n",
    "    def map_location_normalized(self, loc_str):\n",
    "        if pd.isna(loc_str) or not isinstance(loc_str, str):\n",
    "            return None\n",
    "        \n",
    "        found_provinces = set()\n",
    "        found_cities = [] # List of dicts\n",
    "        \n",
    "        # 1. Search for Provinces\n",
    "        for key, full_name in self.provinces_map.items():\n",
    "            if key in loc_str:\n",
    "                found_provinces.add(full_name)\n",
    "                \n",
    "        # 2. Search for Cities\n",
    "        for key, info in self.cities_map.items():\n",
    "            if key in loc_str:\n",
    "                match_info = info.copy()\n",
    "                match_info['key'] = key\n",
    "                found_cities.append(match_info)\n",
    "                \n",
    "        # 3. Consolidate and Remove Redundancy\n",
    "        \n",
    "        # 3a. Identify implied provinces from found cities\n",
    "        implied_provinces = set()\n",
    "        for c in found_cities:\n",
    "            if pd.notna(c['province']):\n",
    "                implied_provinces.add(c['province'])\n",
    "                \n",
    "        # 3b. Remove found provinces if they are implied by the cities\n",
    "        temp_provinces = set()\n",
    "        for p in found_provinces:\n",
    "            if p not in implied_provinces:\n",
    "                temp_provinces.add(p)\n",
    "        \n",
    "        # 3c. Remove Broad Terms if Specific Terms are present\n",
    "        all_present_specific_provinces = temp_provinces.union(implied_provinces)\n",
    "        \n",
    "        final_provinces = set()\n",
    "        for p in temp_provinces:\n",
    "            is_redundant_broad = False\n",
    "            if p in self.BROAD_TERMS_MAP:\n",
    "                # Check if any specific term for this broad term is present\n",
    "                for specific in self.BROAD_TERMS_MAP[p]:\n",
    "                    if specific in all_present_specific_provinces:\n",
    "                        is_redundant_broad = True\n",
    "                        break\n",
    "            \n",
    "            if not is_redundant_broad:\n",
    "                final_provinces.add(p)\n",
    "                \n",
    "        # 4. Format Output\n",
    "        final_results = set()\n",
    "        \n",
    "        # Add Remaining Provinces\n",
    "        for p in final_provinces:\n",
    "            final_results.add(p)\n",
    "            \n",
    "        # Add Cities (Format: \"Province City\" or \"City\")\n",
    "        for c in found_cities:\n",
    "            full_city = c['full']\n",
    "            province = c['province']\n",
    "            \n",
    "            if pd.notna(province):\n",
    "                final_results.add(f\"{province} {full_city}\")\n",
    "            else:\n",
    "                final_results.add(full_city)\n",
    "                \n",
    "        if not final_results:\n",
    "            return None\n",
    "            \n",
    "        return ', '.join(sorted(list(final_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/full_df_final.csv with 19141 rows.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "input_file = 'data/full_df_final.csv'\n",
    "output_file = 'data/event_loc_normalized.csv'\n",
    "\n",
    "if os.path.exists(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loaded {input_file} with {len(df)} rows.\")\n",
    "else:\n",
    "    print(f\"Error: {input_file} not found.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization maps built successfully.\n",
      "Normalizing 'event_loc' column...\n",
      "Sample of normalized locations:\n",
      "                        event_loc              event_loc_normalized\n",
      "0                       평안북도, 신의주                         평안북도 신의주시\n",
      "1                             NaN                              None\n",
      "2                             개성시                               개성시\n",
      "3             평양시, 혜산시, 원산시, 사리원시  강원도 원산시, 양강도 혜산시, 평양시, 황해북도 사리원시\n",
      "4  평안남도, 황해북도, 강원도, 양강도, 나선시, 개성시    강원도, 개성시, 나선시, 양강도, 평안남도, 황해북도\n",
      "5                            평안남도                              평안남도\n",
      "6                             평양시                               평양시\n",
      "7                             양강도                               양강도\n",
      "8              평양시, 함경도, 평안도, 양강도                양강도, 평안도, 평양시, 함경도\n",
      "9                        강원도, 고성시                               강원도\n",
      "Saved normalized data to data/full_df_final_normalized.csv\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # Initialize Normalizer\n",
    "    normalizer = LocationNormalizer()\n",
    "    \n",
    "    # Apply normalization\n",
    "    print(\"Normalizing 'event_loc' column...\")\n",
    "    # We use a lambda to apply the function, handling potential non-string values gracefully (though the method handles them)\n",
    "    df['event_loc_normalized'] = df['event_loc'].apply(normalizer.map_location_normalized)\n",
    "    \n",
    "    # Check some results\n",
    "    print(\"Sample of normalized locations:\")\n",
    "    print(df[['event_loc', 'event_loc_normalized']].head(10))\n",
    "    \n",
    "    # Update the original column if desired, or keep as new column. \n",
    "    # The user asked to \"normalize event_loc\", so we should probably update the column.\n",
    "    # However, keeping the original for comparison is often good practice in 'prep' files.\n",
    "    # But to fulfill \"normalize event_loc\", I will update the column in the saved file.\n",
    "    \n",
    "    df['event_loc'] = df['event_loc_normalized']\n",
    "    df.drop(columns=['event_loc_normalized'], inplace=True)\n",
    "    \n",
    "    # Save to new file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved normalized data to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
